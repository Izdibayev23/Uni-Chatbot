import pandas as pd
from datasets import Dataset
from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments, GenerationConfig
from sklearn.model_selection import train_test_split
from transformers import EarlyStoppingCallback

# Load and preprocess the dataset
df = pd.read_csv('updated_merged.csv')
df = df.rename(columns={'question': 'input_text', 'answer': 'target_text'})

# Check the column names
print("Columns in DataFrame:", df.columns)

# Convert DataFrame to Hugging Face Dataset
dataset = Dataset.from_pandas(df)

# Split dataset into train and validation sets
dataset = dataset.train_test_split(test_size=0.3)
train_dataset = dataset['train']
val_dataset = dataset['test']

# Tokenizer and model
model_name = "facebook/bart-base"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

def preprocess_function(examples):
    inputs = [f"question: {q}" for q in examples['input_text']]
    targets = [f"{a}" for a in examples['target_text']]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    labels = tokenizer(targets, max_length=512, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply the preprocessing function
train_dataset = train_dataset.map(preprocess_function, batched=True)
val_dataset = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results4',
    num_train_epochs=100,  # Reduce epochs to a reasonable number to prevent overfitting
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    save_total_limit=2,
    evaluation_strategy="epoch",  # Ensure both evaluation and save strategies are set to "epoch"
    save_strategy="epoch",        # Set save strategy to "epoch"
    load_best_model_at_end=True,  # Load the best model at the end of training
    learning_rate=3e-4,  # Typical starting learning rate for transformers
    gradient_accumulation_steps=2,  # Helps with small batch sizes
    fp16=True,  # Use mixed precision training if supported by GPU
    metric_for_best_model="eval_loss",
    greater_is_better=False
)

# Define Trainer with early stopping
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
)

# Train the model
trainer.train()

# Save the model
model.save_pretrained('./fine_tuned_bart')
tokenizer.save_pretrained('./fine_tuned_bart')

# Define the generation configuration
generation_config = GenerationConfig(
    early_stopping=True,
    num_beams=4,
    no_repeat_ngram_size=3,
    forced_bos_token_id=0,
    forced_eos_token_id=2
)

# Save the generation config
generation_config.save_pretrained('./fine_tuned_bart')
